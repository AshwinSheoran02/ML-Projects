{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self , N , hidden_layer_size , lr, A_function ,max_iter , Weight_fun , batch_size ) :\n",
    "        self.N = N\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lr = lr ## learning rate\n",
    "        self.A_function = A_function\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iter = max_iter ## number of epochs\n",
    "        self.Weight_fun = Weight_fun  # Weight initialization function\n",
    "        self.layers = []\n",
    "\n",
    "\n",
    "\n",
    "# ##### Helper functions  for part A\n",
    "    def forward(self , X):   ## . helper function for going into next layer\n",
    "        W1 = self.random_init_function(self.hidden_layer_size[0] , self.hidden_layer_size[1])   \n",
    "        ## Weight matrix using function\n",
    "        A_prev = X   ## initial value at the previous layer\n",
    "            \n",
    "        Z = np.dot(A_prev, W1 )     ## Multiply with weights to get probability\n",
    "\n",
    "        if(self.A_function == \"relu\"):\n",
    "            A = self.relu(Z)\n",
    "\n",
    "        elif (self.A_function == \"tanh\"):\n",
    "            A = self.tanh(Z)\n",
    "\n",
    "        elif (self.A_function == \"linear\"):\n",
    "            A = self.linear(Z)\n",
    "\n",
    "        elif (self.A_function == \"sigmoid\"):\n",
    "            A = self.sigmoid(Z)\n",
    "\n",
    "        return A \n",
    "    \n",
    "    def loss_function(self, A, y):  ## Helper function\n",
    "        log_val = - np.log(A[np.arange(len(y)), y.argmax(axis=1)])\n",
    "        loss = np.sum(log_val)/ len(y)\n",
    "        return loss\n",
    "\n",
    "\n",
    "## Part A\n",
    "#   \n",
    "##\n",
    "    def fit(self , X , Y):\n",
    "        train_loss = [] ## array for storing the train loss\n",
    "        for epoch in self.max_iter:\n",
    "            train_batch_loss = []\n",
    "\n",
    "            for batch in self.batch_size:\n",
    "                A = self.forward(X) ## Moving to next nueron\n",
    "                curr_loss = self.loss_function(A , Y[batch]) ## Calculating loss using the helper function\n",
    "                train_batch_loss.append(curr_loss) \n",
    "            \n",
    "            train_loss.append( train_batch_loss)\n",
    "        return train_loss\n",
    "\n",
    "\n",
    "\n",
    "    def predict_proba(self , X):\n",
    "        y_proba = []        \n",
    "        \n",
    "        output = X;   \n",
    "        for layer in self.layers:\n",
    "            output = self.forward(output) ## Going forward through all the layers\n",
    "            y_proba.append(output)\n",
    "        y_proba = self.softmax(y_proba)     ## using softmax as this is the last layer\n",
    "        return np.array(y_proba) ## class wise probability \n",
    "\n",
    "\n",
    "    def predict(self , X):\n",
    "        y_pred = []\n",
    "        y_proba = self.predict_proba(X)\n",
    "        for i in range(len(y_proba)):\n",
    "            y_pred.append(np.argmax(y_proba[i])) ##Predicting the y by passing the X_train into the predict_proba function  \n",
    "        return np.array(y_pred)                     ## and getting the y pred\n",
    "\n",
    "    def score(self , X , Y):\n",
    "        y_pred = self.predict(X) ## predicting the y\n",
    "        return np.mean(y_pred== Y)   ## returning he score\n",
    "\n",
    "\n",
    "### B Part\n",
    " \n",
    "    ## Here gradients are partial derivatives of the functions\n",
    "\n",
    "    def relu(self, X):\n",
    "        return X * (X>=0) ### return max(0,X)\n",
    "\n",
    "    def relu_grad(self, X):\n",
    "        return 1*(X>=0)     ## return 1 if x>0 else 0\n",
    "\n",
    "    def leaky_relu(self, X):\n",
    "        return np.maximum(0.1 * X, X)      ## return 0.01 * X if x<0 else X\n",
    "\n",
    "    def leaky_relu_grad(self, X):\n",
    "        if X>0 :\n",
    "            return X\n",
    "        else :\n",
    "            return 0.01*X        ## return 0.01 if x<0 else X\n",
    "    \n",
    "    def linear(self, X):   ## in linear we return X only\n",
    "        return X\n",
    "\n",
    "    def linear_grad(self, X):   ## the matrix with all 1s instead of values of x\n",
    "        return np.ones(X.shape)\n",
    "    \n",
    "    def sigmoid(self, X):\n",
    "        return 1/(1+np.exp(-X)) ## returning this value using the definition of sigmoid function\n",
    "\n",
    "    def sigmoid_grad(self, X):\n",
    "        return self.sigmoid(X) * (1-self.sigmoid (X)) ## We get this result on taking derivative of the sigmoid function\n",
    "\n",
    "    def tanh(self, X):   # Compute hyperbolic tangent element-wise ( (e^X - e^-X)/(e^X + e^-X) )\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def tanh_grad(self, X):\n",
    "        return 1 - self.tanh(X)*self.tanh(X)  ## derivative of tanh(x) is 1 - tanh2(x)\n",
    "\n",
    "    def softmax(self, X):\n",
    "        exp = np.exp(X)\n",
    "        return exp/(np.sum(exp))        ## softmax is a normalized exponential function\n",
    "\n",
    "    def softmax_grad(self, X):\n",
    "        result_matrix = np.diag(X) ## constructing jacobian matrix of softmax as it is the maxtrix of partial derivatives\n",
    "\n",
    "        for i in range(len(result_matrix)):\n",
    "            for j in range(len(result_matrix)):\n",
    "                if i == j:\n",
    "                    result_matrix[i][j] = (1-X[i])\n",
    "                else: \n",
    "                    result_matrix[i][j] = -X[i]*X[j]\n",
    "        return result_matrix            \n",
    "    \n",
    "\n",
    "\n",
    "#### Part C\n",
    "\n",
    "    ## Here shape = (self.hidden_layer_size[0],self.hidden_layer_size[1])\n",
    "\n",
    "    def zero_init_function(self, shape):\n",
    "        weight = np.zeros(shape)    ## returns weight with just zero initialization \n",
    "        return weight\n",
    "\n",
    "    def random_init_function(self, shape):\n",
    "        weight = np.random.rand(shape[0], shape[1])     ## return weights with random initialization\n",
    "        return weight\n",
    "\n",
    "    def normal_init_function(self, shape):\n",
    "        weight = np.random.normal(0 , 1 , size = shape , scale = 0.01)    \n",
    "        ##samples from the parameterized normal distribution (Guassian)\n",
    "        ## Mean 0 , Variance 1\n",
    "        return weight\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aad85d0c0b1d7e2993ce6f051416dbc063ba10d26d1f30ddc77f61941d42f40e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
